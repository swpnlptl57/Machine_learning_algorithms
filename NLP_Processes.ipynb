{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"Harry. It was a great pleasure to discuss with you on recent issues. Hope your team would be able to resolve and come up with propoer solutions by Monday. I need your project plan and actionables by today evening on my desk. You are free to experiment and come with results without fail by Monday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harry.', 'It was a great pleasure to discuss with you on recent issues.', 'Hope your team would be able to resolve and come up with propoer solutions by Monday.', 'I need your project plan and actionables by today evening on my desk.', 'You are free to experiment and come with results without fail by Monday']\n",
      "['Harry', '.', 'It', 'was', 'a', 'great', 'pleasure', 'to', 'discuss', 'with', 'you', 'on', 'recent', 'issues', '.', 'Hope', 'your', 'team', 'would', 'be', 'able', 'to', 'resolve', 'and', 'come', 'up', 'with', 'propoer', 'solutions', 'by', 'Monday', '.', 'I', 'need', 'your', 'project', 'plan', 'and', 'actionables', 'by', 'today', 'evening', 'on', 'my', 'desk', '.', 'You', 'are', 'free', 'to', 'experiment', 'and', 'come', 'with', 'results', 'without', 'fail', 'by', 'Monday']\n"
     ]
    }
   ],
   "source": [
    "print(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph1 =\"\"\"Welcome to the 5th annual gathering of the American Car Enthusiasts. It is great to see so many enthusiastic men \n",
    "             \n",
    "            and women here celebrating the genius\n",
    "            \n",
    "            and innovation of American car manufacturers over the years. \n",
    "            \n",
    "            It has been a long time since Henry Ford got the ball rolling and look how far we've come!\n",
    "             \n",
    "            Are you happy to be here?\n",
    "\n",
    "            ACE is pleased to host this conference in Detroit this year where we will have the opportunity to see the best cars\n",
    "            \n",
    "            America has had to offer, past and present!\n",
    "\n",
    "            I know that many of you have traveled from quite a distance and I'd like to acknowledge our members from Alaska, \n",
    "            \n",
    "            Hawaii and even Guam who have made this long trip \n",
    "            \n",
    "            to participate in the largest, most extensive ACE conference we have ever had.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph1)\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])# list of words from each and every sentences\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]# if word in sentence belongs to stop\n",
    "    # word we are going to remove it other wise we will keep stem words\n",
    "    sentences[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'know', 'mani', 'travel', 'quit', 'distanc', 'I', \"'d\", 'like', 'acknowledg', 'member', 'alaska', ',', 'hawaii', 'even', 'guam', 'made', 'long', 'trip', 'particip', 'largest', ',', 'extens', 'ace', 'confer', 'ever', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcom 5th annual gather american car enthusiast .', 'It great see mani enthusiast men women celebr geniu innov american car manufactur year .', \"It long time sinc henri ford got ball roll look far 've come !\", 'are happi ?', 'ace pleas host confer detroit year opportun see best car america offer , past present !', \"I know mani travel quit distanc I 'd like acknowledg member alaska , hawaii even guam made long trip particip largest , extens ace confer ever .\"]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)# paragraph after removing the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limitation of stemming is that produced intermediate word may not have any meaning eg --- extens ^^^^ in above output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lammentizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer # converts the parapgraph into meaning ful base words unlike stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph2 = \"\"\"Welcome to the 5th annual gathering of the American Car Enthusiasts. It is great to see so many enthusiastic men \n",
    "             \n",
    "            and women here celebrating the genius\n",
    "            \n",
    "            and innovation of American car manufacturers over the years. \n",
    "            \n",
    "            It has been a long time since Henry Ford got the ball rolling and look how far we've come!\n",
    "             \n",
    "            Are you happy to be here?\n",
    "\n",
    "            ACE is pleased to host this conference in Detroit this year where we will have the opportunity to see the best cars\n",
    "            \n",
    "            America has had to offer, past and present!\n",
    "\n",
    "            I know that many of you have traveled from quite a distance and I'd like to acknowledge our members from Alaska, \n",
    "            \n",
    "            Hawaii and even Guam who have made this long trip \n",
    "            \n",
    "            to participate in the largest, most extensive ACE conference we have ever had.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome to the 5th annual gathering of the American Car Enthusiasts.', 'It is great to see so many enthusiastic men \\n             \\n            and women here celebrating the genius\\n            \\n            and innovation of American car manufacturers over the years.', \"It has been a long time since Henry Ford got the ball rolling and look how far we've come!\", 'Are you happy to be here?', 'ACE is pleased to host this conference in Detroit this year where we will have the opportunity to see the best cars\\n            \\n            America has had to offer, past and present!', \"I know that many of you have traveled from quite a distance and I'd like to acknowledge our members from Alaska, \\n            \\n            Hawaii and even Guam who have made this long trip \\n            \\n            to participate in the largest, most extensive ACE conference we have ever had.\"]\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph2)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lammentizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])# list of words from each and every sentences\n",
    "    words = [lammentizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    \n",
    "    sentences[i] = ' '.join(words)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'know', 'many', 'traveled', 'quite', 'distance', 'I', \"'d\", 'like', 'acknowledge', 'member', 'Alaska', ',', 'Hawaii', 'even', 'Guam', 'made', 'long', 'trip', 'participate', 'largest', ',', 'extensive', 'ACE', 'conference', 'ever', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome 5th annual gathering American Car Enthusiasts .', 'It great see many enthusiastic men woman celebrating genius innovation American car manufacturer year .', \"It long time since Henry Ford got ball rolling look far 've come !\", 'Are happy ?', 'ACE pleased host conference Detroit year opportunity see best car America offer , past present !', \"I know many traveled quite distance I 'd like acknowledge member Alaska , Hawaii even Guam made long trip participate largest , extensive ACE conference ever .\"]\n"
     ]
    }
   ],
   "source": [
    "print( sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph3 = \"\"\"\"Among the activities we have planned are workshops about the evolution of individual car models such as the Thunderbird and the Mustang as well as demonstrations of the performance characteristics of our newest American offerings.\n",
    "\n",
    "Please consult your conference brochures for specific events. In the exhibition hall you will be able to visit each of the manufacturers of our fine American automobiles as well as the booths of our valued partners who support the industry in various ways. Don't forget to enter all the raffles and drawings for the latest accessories, tickets to NASCAR races and even a 2007 Corvette.\n",
    "\n",
    "Folks, you are lucky to be here this year because we have some tremendous events planned for each day of the conference.\n",
    "\n",
    "Tonight is our opening banquet with our keynote speaker, Lee Roker. Tomorrow we have the Winner's Circle cocktail hour hosted by many of our fine sponsors. On closing day we will have a Texas style barbecue starting at 1 o'clock on the terrace outside the exhibition hall.\n",
    "\n",
    "I know most of you are already members of ACE but for those of you who aren't, I urge you to fill out a membership application so that you can enjoy all the benefits that our members enjoy here, including discounts for the field trips we have planned and a subscription to our fine newsletter, ACE Quarterly.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the texts\n",
    "import re # import set of regular expressions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an object\n",
    "ps = PorterStemmer()\n",
    "wl = WordNetLemmatizer()\n",
    "sentences =  nltk.sent_tokenize(paragraph3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i]) # removing punctuation comma etc etc\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wl.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know most of you are already members of ACE but for those of you who aren't, I urge you to fill out a membership application so that you can enjoy all the benefits that our members enjoy here, including discounts for the field trips we have planned and a subscription to our fine newsletter, ACE Quarterly.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "know already member ace urge fill membership application enjoy benefit member enjoy including discount field trip planned subscription fine newsletter ace quarterly\n"
     ]
    }
   ],
   "source": [
    "print(review[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['know', 'already', 'member', 'ace', 'urge', 'fill', 'membership', 'application', 'know already member ace urge fill membership application enjoy benefit member enjoy including discount field trip planned subscription fine newsletter ace quarterly', 'benefit', 'member', 'enjoy', 'including', 'discount', 'field', 'trip', 'planned', 'subscription', 'fine', 'newsletter', 'ace', 'quarterly']\n"
     ]
    }
   ],
   "source": [
    "print(corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the bag of words model\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')# creates the bag of words\n",
    "X = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0\n",
      "  0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 1 1 1 1 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2 0 0 0 0 1 1 0 0 0 0\n",
      "  0 1 0 0 0 1 0 0 0 0 2 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 1 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term frequency - inverse derived frequency\n",
    "#TF = no of repetations of words in sentence / no of words in sentence \n",
    "\n",
    "#IDF = log(no of sentence / no of  sentence containing the words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph4=\"\"\"\"Good evening all. It's great to have you here. Most of you know my feeling on birthdays. Generally I say, what's the big deal?\n",
    "\n",
    "By the time you've had over thirty, there should be a cease and desist order against them.\n",
    "\n",
    "They're not unusual. Everybody has them and at the same rate as everybody else - one a year. They happen whether you want them to or not.\n",
    "\n",
    "Believe me, I know. I've had quite a few and looking around this room I can see it's the same for others as well.\n",
    "\n",
    "So why are we here?\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the texts\n",
    "import re # import set of regular expressions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an object\n",
    "ps = PorterStemmer()\n",
    "wl = WordNetLemmatizer()\n",
    "sentences =  nltk.sent_tokenize(paragraph4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i]) # removing punctuation comma etc etc\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wl.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cv =TfidfVectorizer(stop_words='english')# creates the bag of words\n",
    "X = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Good evening all.', \"It's great to have you here.\", 'Most of you know my feeling on birthdays.', \"Generally I say, what's the big deal?\", \"By the time you've had over thirty, there should be a cease and desist order against them.\", \"They're not unusual.\", 'Everybody has them and at the same rate as everybody else - one a year.', 'They happen whether you want them to or not.', 'Believe me, I know.', \"I've had quite a few and looking around this room I can see it's the same for others as well.\", 'So why are we here?\"']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good evening', 'great', 'know feeling birthday', 'generally say big deal', 'time thirty cease desist order', 'unusual', 'everybody rate everybody else one year', 'happen whether want', 'believe know', 'quite looking around room see others well', '']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.70710678 0.         0.         0.         0.70710678 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.60515873 0.         0.         0.\n",
      "  0.         0.         0.60515873 0.         0.         0.\n",
      "  0.         0.51726765 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.5        0.         0.         0.5        0.\n",
      "  0.         0.         0.         0.5        0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.4472136  0.         0.4472136\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.4472136  0.         0.\n",
      "  0.         0.         0.4472136  0.4472136  0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.81649658 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.40824829\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.40824829]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.70710678 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.70710678\n",
      "  0.        ]\n",
      " [0.76014955 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.64974816 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.57735027 0.         0.57735027 0.\n",
      "  0.57735027 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
