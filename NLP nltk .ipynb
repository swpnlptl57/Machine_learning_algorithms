{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download()\n",
    "print(\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stem.LancasterStemmer\n",
    "stem.PorterStemmer\n",
    "stem.ISRIStemmer\n",
    "stem.RegexpStemmer\n",
    "#Roll 51 to 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "s = input(\"Enter a value\")\n",
    "print('Porter Result')\n",
    "res = porter.stem(s)\n",
    "print(res)\n",
    "print('Lancaster result')\n",
    "res = lancaster.stem(s)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"It has been all about lock-down this year 2020. People have fearfully lived all the time. We expect a better and cheerful new year. \"\n",
    "sentence = \"It has been all about lock-down this year 2020 People have fearfully lived all the time We expect a better and cheerful new year. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sent = sent_tokenize(sentence)\n",
    "print(token_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_word = word_tokenize(sentence)\n",
    "print(token_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "word = input(\"Enter a word : \")\n",
    "res = wnl.lemmatize(word)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"Harry. It was a great pleasure to discuss with you on recent issues. Hope your team would be able to resolve and come up with propoer solutions by Monday. I need your project plan and actionables by today evening on my desk. You are free to experiment and come with results without fail by Monday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nltk.sent_tokenize(paragraph)\n",
    "print(dataset)\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find out the most freq words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagw = {}\n",
    "for sen in dataset:\n",
    "    words = nltk.word_tokenize(sen)\n",
    "    for word in words:\n",
    "        if word not in bagw.keys():\n",
    "            bagw[word] = 1\n",
    "        else:\n",
    "            bagw[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bagw)\n",
    "print(type(bagw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW - heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "fr_words = heapq.nlargest(10, bagw, key=bagw.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fr_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# tokens = RegexpTokenizer(r'[a-zA-Z0-9]+',gaps=False)\n",
    "# cv = CountVectorizer(stop_words='english',tokenizer=tokens())\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "# word_freq = cv.fit_transform(['word test word apple word test 111 111 111 111'])\n",
    "word_freq = cv.fit_transform([paragraph])\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=RegexpTokenizer(r'[a-zA-Z0-9]+',gaps=False)\n",
    "tk=tokens.tokenize(paragraph)\n",
    "print(tk)\n",
    "cv=CountVectorizer(stop_words='english')\n",
    "word_freq=cv.fit_transform(tk)\n",
    "print(word_freq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
